---
title: "Chapter 7 Moving Beyond Linearity"
output: html_notebook
---

## __Conceptual__
__1.__ __(a)__


__(b)__

## __Applied__
```{r}
library(ISLR2)
library(boot)


attach(Wage)
```

__6.__ __(a)__

```{r}
set.seed(1)
spl = sample(1:nrow(Wage), size=0.7 * nrow(Wage))
train = Wage[spl,]
test = Wage[-spl,]
train_rss = c(1:8)
test_rss = c(1:8)

for (i in 1:8) {
  lm.fit = lm(wage~poly(age,i), data = train)
  lm.pred_train = predict.lm(lm.fit, train)
  lm.pred = predict.lm(lm.fit, test)
  train_rss[i] = sum((lm.pred_train - train$wage)**2) / nrow(train)
  test_rss[i] = sum((lm.pred - test$wage)**2) / nrow(test)
  
}
```

```{r}
plot(test_rss, type="b", xlab="degree", ylab="Error")
```

```{r}
# Cross validation to choose degree of polynomial.
set.seed(1)
cv.error.10 = rep(0,10)
for (i in 1:10) {
  glm.fit=glm(wage~poly(age,i),data=Wage)
  cv.error.10[i]=cv.glm(Wage,glm.fit,K=10)$delta[1]
}
cv.error.10
plot(cv.error.10, type="b", xlab="Degree", ylab="CV Error")
```
ANOVA TEST
```{r}

lm.fit1=lm(wage~poly(age,1),data=Wage)
lm.fit2=lm(wage~poly(age,2),data=Wage)
lm.fit3=lm(wage~poly(age,3),data=Wage)
lm.fit4=lm(wage~poly(age,4),data=Wage)
lm.fit5=lm(wage~poly(age,5),data=Wage)

anova(glm.fit1, glm.fit2, glm.fit3, glm.fit4, glm.fit5)

```
The results show a decrease in statistical significance of model p-values for higher order polynomials.
Specifically comparison between 4 and 5th degree models barely higher than 5% p-value.
We would probably choose a order 3 or 4 polynomial to explain wage.

```{r}

agelims=range(age)
age.grid=seq(from=agelims[1],to=agelims[2])

# Predictions.
preds=predict(lm.fit4,newdata=list(age=age.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)


plot(age, wage,cex=.5)
title("Polynomial fit using degree 4")
lines(age.grid,preds$fit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd =1,col="blue",lty =3)
```

__(b)__


```{r}
# pick cutting points for the step function
table(cut(age, 4))

fit = glm(wage ~ cut(age, 4), data = Wage)
coef(summary(fit))

```




```{r}
# Cross validation to choose the optimal number of cuts
library(boot)

set.seed(1)
cv.error.cuts = rep(NA,19)
for (i in 2:20) {
  Wage$age.cut = cut(age, i)
  glm.fit=glm(wage ~ age.cut, data = Wage)
  cv.error.cuts[i]=cv.glm(Wage, glm.fit, K=10)$delta[1]
}

cv.error.cuts
plot(cv.error.cuts, type="b", xlab="Degree", ylab="CV Error")
```

After 8 cuts there is no significant drop in CV error that justifies the increase in complexity.


```{r}
agelims=range(age)
age.grid=seq(from=agelims[1],to=agelims[2])

# Predictions.
lm.fit = lm(wage ~ cut(age, 8), data=Wage)
preds=predict(lm.fit,newdata=list(age=age.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)


plot(age, wage, cex=.5, col="darkgrey")
title("Step function using 8 cuts")
lines(age.grid,preds$fit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd =1,col="blue",lty =3)
```

__(7)__
__(a)__
```{r}
summary(Wage)
```
## explore other relationships with explained wage variable
we will ignore region as there is only 1 possible value
we will explore year (to account for inflation), education, health, jobclass 
```{r}
# explore other relationships with wage variable

library(splines)
fit = smooth.spline(wage ~ age, df=3)
fit2 = smooth.spline(wage ~ age, cv=TRUE)
fit2$df
plot (age , wage , cex = .5, col = " darkgrey ")
lines (fit , col = " red ", lwd = 2)
lines (fit2 , col = " blue ", lwd = 2)
legend ("bottomright", legend = c("3", fit2$df),
col = c("red", "blue"), lty = 1, lwd = 2, cex = .8)

```



```{r}
boxplot(wage ~ education)
boxplot(wage ~ maritl)
boxplot(wage ~ jobclass)
```
### education:
There is a general rising trend with outliers in education higher than HS Grad. In general the outliers are further away from the main bell curve for lower levels of education.

### marital status
High amount of outliers with married and never married people.
Married people seem to have higher income. 

### job class
Information seems to have slightly higher mean value and deviation

```{r}
library(gam)
```
```{r}
fit = gam(wage ~ jobclass)
summary(fit)
coef(fit)
plot ( fit , se = TRUE , col = " blue ")
```


```{r}
fit = gam(wage ~ s(age, 4) + jobclass + education)
summary(fit)
par ( mfrow = c(1, 3))
plot ( fit , se = TRUE , col = " blue ")
#plot(maritl , wage , cex = .5, col = " darkgrey ")
#lines(fit , col = " red ", lwd = 2)
#legend("bottomright", legend = c(fit1$df),
#col = c("red"), lty = 1, lwd = 2, cex = .8)
```



__(8)__
```{r}
detach(Wage)
attach(Auto)
help(Auto)
```
__(a)__
```{r}
pairs(Auto)

```
The Auto dataset looks to have non-linear relationships with displacement, horsepower, weight and maybe year from the pair plots displayed above

```{r}
plot(displacement, mpg)
plot(horsepower, mpg)
plot(weight, mpg)
plot(year, mpg)

```


```{r}
fit = smooth.spline(mpg ~ horsepower, cv=TRUE)

fit$df

plot (horsepower, mpg, cex = .5, col = " darkgrey ")
lines (fit , col = "red", lwd = 2)
legend ("topright", legend = c(fit2$df),
col = c("red"), lty = 1, lwd = 2, cex = .8)
```



```{r}
fit = smooth.spline(mpg ~ displacement, cv=TRUE)

fit$df

plot (displacement, mpg, cex = .5, col = " darkgrey ")
lines (fit , col = "red", lwd = 2)
legend ("topright", legend = c(fit2$df),
col = c("red"), lty = 1, lwd = 2, cex = .8)
```


```{r}
fit = smooth.spline(mpg ~ displacement, cv=TRUE)
cv.error = rep(NA, 10)
for (i in 1:10){
  fit = glm(mpg~displacement, data=Auto)
  cv.error[i] = cv.glm(Auto, fit, K=10)$delta[1]
}
plot(cv.error, type = 'b')
```


```{r}
fit1 = glm(mpg ~ displacement)
fit3 = glm(mpg ~ poly(displacement, 3))
fit4 = glm(mpg ~ poly(displacement, 4))
displim = range(displacement)
disp.grid = seq(displim[1], displim[2])
preds1 = predict(fit1, newdata = list(displacement=disp.grid))
preds3 = predict(fit3, newdata = list(displacement=disp.grid))
preds4 = predict(fit4, newdata = list(displacement=disp.grid))
plot (displacement, mpg, cex = .5, col = " darkgrey ")
lines (preds1, col = "red", lwd = 2)
lines (preds3, col = "blue", lwd = 2)
lines (preds4, col = "green", lwd = 2)
legend ("topright", legend = c("linear fit", "cubic fit", "quartic fit"),
col = c("red", "blue", "green"), lty = 1, lwd = 2, cex = .8)
```

__(9)__
```{r}
detach(Auto)
attach(Boston)
help(Boston)
```
__(a)__ __(b)__ __(c)__
```{r}
cv.error = rep(NA, 10)
for (i in 1:10){
  glm.fit = glm(nox~poly(dis,i))
  cv.error[i] = cv.glm(Boston, glm.fit, K=10)$delta[1]
}
plot(cv.error, type='b')
```
2nd degree polynomial seems to be optimal since there is no considerable improvement for higher order polynomials.

```{r}
glm.fit1 = glm(nox~poly(dis, 1), data = Boston)
glm.fit2 = glm(nox~poly(dis, 2), data = Boston)
glm.fit3 = glm(nox~poly(dis, 3), data = Boston)
glm.fit4 = glm(nox~poly(dis, 4), data = Boston)
glm.fit5 = glm(nox~poly(dis, 5), data = Boston)
glm.fit6 = glm(nox~poly(dis, 6), data = Boston)

dislim = range(dis)
dis.grid = seq(dislim[1], dislim[2])

pred1 = predict(glm.fit1, newdata=list(dis=dis.grid))
pred2 = predict(glm.fit2, newdata=list(dis= dis.grid))
pred3 = predict(glm.fit3, newdata=list(dis= dis.grid))
pred4 = predict(glm.fit4, newdata=list(dis= dis.grid))
pred5 = predict(glm.fit5, newdata=list(dis= dis.grid))
pred6 = predict(glm.fit6, newdata=list(dis= dis.grid))

plot(dis, nox, col="darkgrey", cex=.5)
lines(pred1, col="blue", lwd=2)
lines(pred2, col="green", lwd=2)
lines(pred3, col="red", lwd=2)
lines(pred4, col="violet", lwd=2)

```
__(d)__
```{r}
lm.fit = lm(nox ~ bs(dis, df=4), data=Boston)

preds = predict(lm.fit, newdata=list(dis=dis.grid), se=T)

attr(bs(dis, df=4), "knots")
summary(lm.fit)


plot(dis, nox, col="darkgray", cex=.5)
lines(preds$fit, col="blue", lwd=2)
lines(preds$fit+2*preds$se, lty="dashed")
lines(preds$fit-2*preds$se, lty="dashed")

```
- knots are automatically selected automatically at uniform quantiles (1 knot means 50th quantile)
- summary() of the model shows that the model is statistically significant.
- The fit is similar to the 3rd degree polynomial fit.

__(e)__
```{r}
rss = rep(NA, 15)
for (i in 3:17){
  lm.fit = lm(nox ~ bs(dis, df=i), data=Boston)
  preds = predict(lm.fit, newdata=Boston)
  rss[i] = sum((preds - nox)**2)
}
plot(rss)

```
After 10 degrees of freedom there isnt a noticeable reduction in train RSS.


```{r}
lm.fit = lm(nox ~ bs(dis, df=10), data=Boston)
pred = predict(lm.fit, newdata=list(dis=dis.grid))
plot(dis, nox, col="darkgray", cex=.5)
lines(pred, col="blue", lwd=2)
```
__(f)__


```{r}
rss.train = rep(NA, 17)
rss.test = rep(NA, 17)
for (i in 3:19){
  glm.fit = glm(nox ~ bs(dis, df=i), data=Boston)
  preds = predict(glm.fit, newdata=Boston)
  rss.train[i-2] = mean((preds - nox)**2)
  
  rss.test[i-2] = cv.glm(Boston, glm.fit, K=10)$delta[1]
  
}
rss.lim = c(min(min(rss.test), min(rss.train)), max(max(rss.test), max(rss.train)))
plot(3:19, rss.test, type='b', col="orange", ylim=c(rss.lim[1], rss.lim[2]), ylab="error")
lines(3:19, rss.train, type='b', col="blue", lwd=2)
legend("bottomleft", legend=c("rss.train", "rss.test"), col = par("blue", "orange"), lty = 1, lwd = 2, cex = .8)

```

Cross validation error shows a significant drop in rss error for 5-6 degrees of freedom (orange). There is not sufficient evidence to choose a greater value than 6 degrees of freedom.



__(10)__

```{r}
detach(Boston)
attach(College)

require(caTools)
```


```{r}
college.sample = sample.split(College$Outstate, SplitRatio = 0.80)
college.train = College[college.sample,]
college.test = College[college.sample==FALSE,]

```


### step forward feature selection
```{r}
## picking the best 8 variables for model prediction according to step-forward selection
regfit.full = regsubsets(Outstate~., data = college.train, nvmax=17)
regfit.summary = summary(regfit.full)

plot ( regfit.full , scale = "r2")
plot ( regfit.full , scale = "adjr2")
plot ( regfit.full , scale = "Cp")
plot ( regfit.full , scale = "bic")
plot (regfit.summary$adjr2)
plot (regfit.summary$bic)
```
From the adjr2 and bic plot above there seems to top off with 6 variables and there are only small improvements after.


```{r}
# the chosen parameters are:
coef(regfit.full, 6)
```

__(b)__
```{r}
gam.fit = gam(Outstate ~ Private + s(Room.Board, 4) + s(PhD, 4) + perc.alumni + s(Expend, 4) + s(Grad.Rate, 4), data=college.train)
preds = predict(gam.fit, newdata = college.test)
rss = mean((preds-college.test$Outstate)^2)
mse

plot(gam.fit, se=TRUE)
```
__(d)__
### check if there is evidence of a non-linear relationship

```{r}
gam.fit = gam(Outstate ~ Private + Room.Board + PhD + perc.alumni + Expend + Grad.Rate, data=college.train)
gam.fit1 = gam(Outstate ~ Private + Room.Board + PhD + perc.alumni + Expend + s(Grad.Rate, 4), data=college.train)
gam.fit2 = gam(Outstate ~ Private + s(Room.Board, 4) + PhD + perc.alumni + Expend + Grad.Rate, data=college.train)
gam.fit3 = gam(Outstate ~ Private + Room.Board + s(PhD, 4) + perc.alumni + Expend + Grad.Rate, data=college.train)
gam.fit4 = gam(Outstate ~ Private + Room.Board + PhD + s(perc.alumni, 4) + Expend + Grad.Rate, data=college.train)
gam.fit5 = gam(Outstate ~ Private + Room.Board + PhD + perc.alumni + s(Expend, 4) + Grad.Rate, data=college.train)

anova(gam.fit, gam.fit1)
anova(gam.fit, gam.fit2)
anova(gam.fit, gam.fit3)
anova(gam.fit, gam.fit4)
anova(gam.fit, gam.fit5)

```

From the anova results above that compare a base model with linear relationship between variables to different models with non-linearity added to each specific variable we can tell that:

- There is evidence for a non-linear relationship between Grad.Rate and Outstate (p-value of 3.2%)
- PhD and Outstate (p-value of 0.16%)
- same applies for Expend variable p-value < 2.2e-16
- Room.Board barely misses the mark with a p-value > 6%

__(11)__

__(a)__
```{r}
set.seed(5)

# Generated dataset
X1 = rnorm(100, sd=2)
X2 = rnorm(100, sd=sqrt(2))
eps = rnorm(100, sd = 1)
b0 = 5; b1=2.5 ; b2=11.5
Y = b0 +b1*X1 + b2*X2 + eps
```
__(b)(c)__
keeping beta1 fixed estimate beta2

```{r}
beta1 = 10
a = Y - beta1*X1
beta2 = lm(a~X2)$coef[2]
beta2
```


__(d)__
keeping beta2 fixed, estimate a new beta1
```{r}

a = Y - beta2*X2
beta1 = lm(a~X1)$coef[2]
beta1
```
After just 1 iteration beta1 gets much closer to the real value

__(c)__
```{r}
beta.estimates = matrix(NA, nrow=10, ncol=3)
beta1 = 10

for (i in 1:10){
  a = Y - beta1*X1
  beta2 = lm(a~X2)$coef[2]
  a = Y - beta2*X2
  lm.fit = lm(a~X1)
  beta0 = lm.fit$coef[1] 
  beta1 = lm.fit$coef[2]
  beta.estimates[i,1] = beta0
  beta.estimates[i,2] = beta1
  beta.estimates[i,3] = beta2
}

plot(x=1:10, beta.estimates[,1], type='l', ylim=range(1:15), col="black")
#lines(beta.estimates[,1], type='l')
lines(beta.estimates[,2], type='l', col="blue")
lines(beta.estimates[,3], type='l', col="red")

#coefficient estimates from running multiple regression (results from f)
#(Intercept)          X1          X2 
#   4.996049    2.526440   11.536752 
abline(h=4.996049, col="gray", lty=2)
abline(h=2.526440, col="lightblue", lty=2)
abline(h=11.536752, col="pink", lty=2)

```
__(f)__
```{r}
lm.fit = lm(Y ~ X1 + X2)
lm.fit$coef
```
__(g)__
Less tan 4 iterations were needed to get a very good approximation of the coefficients

__(12)__

```{r}
set.seed(5)
n = 1000
p = 100
# Generated dataset
X = matrix(rnorm(n*p, sd=2), ncol=p)
eps = rnorm(n, sd = 1)
betas = rnorm(p, sd=2)+3
b0 = 2.5
Y = b0 + X %*% betas + eps

```




```{r}
beta.estimates = matrix(0, nrow=20, ncol=p)
beta0 = rep(10, 20)

for (i in 1:20){
  for (j in 1:p){
    a = Y - X[,-j] %*% beta.estimates[i,-j]
    lm.fit = lm(a~X[,j])
    beta.estimates[i:20, j] = lm.fit$coef[2]
  }
  beta0[i:20] = lm.fit$coef[1]
}
```




```{r}
plot(x=1:20, beta0, type='l', ylim=range(-5:15), col="black")
beta0
colors = rainbow(10)
for (i in 1:10){
  lines(beta.estimates[,i], col=colors[i], type='l')
  abline(h = betas[i], col=colors[i], lty=2)
}

```
There seems to be convergence after the 3rd/4th iteration



